{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b114ccdc-3517-4fd1-af6e-de8a0fa3e6a8",
   "metadata": {},
   "source": [
    "## Assignment on Support Vector Machines - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec60600c-aace-4db1-a31a-eee83c3bc36e",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b3d60-bc12-44d0-a593-9c6bbdd111b8",
   "metadata": {},
   "source": [
    "In machine learning algorithms, polynomial functions and kernel functions are related through the concept of the kernel trick. The kernel trick allows us to implicitly map data into a higher-dimensional feature space without explicitly calculating the transformed feature vectors.\n",
    "\n",
    "Polynomial functions are a type of function used as a basis for feature mapping. They map the original input space into a higher-dimensional feature space by computing all possible monomial terms up to a specified degree. For example, a 2nd-degree polynomial feature mapping in 2D would transform the features (x, y) into (1, x, y, x², xy, y²).\n",
    "\n",
    "Kernel functions, on the other hand, provide a way to compute the dot product between feature vectors in the higher-dimensional feature space without explicitly computing the transformed feature vectors. They capture the similarity or inner product between two instances in the higher-dimensional space.\n",
    "\n",
    "The relationship between polynomial functions and kernel functions is that polynomial kernels are a specific type of kernel function that computes the inner product of polynomial feature mappings. Instead of explicitly calculating the transformed feature vectors using polynomial functions, we can use polynomial kernels to implicitly compute the dot product in the higher-dimensional space.\n",
    "\n",
    "Mathematically, a polynomial kernel function with degree d can be defined as:\n",
    "\n",
    "K(x, x') = (γ * (x · x') + r)^d\n",
    "\n",
    "where x and x' are the input feature vectors, · denotes the dot product, γ is a scaling factor, r is an additional constant term, and d is the degree of the polynomial.\n",
    "\n",
    "By using a polynomial kernel, we can effectively handle non-linear relationships between features without explicitly computing the high-dimensional feature vectors. The kernel trick allows us to work with the original feature space, avoiding the computational burden and memory requirements associated with explicit feature mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d9b14-e456-4d45-8653-7c13a1eb5276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6543e5e0-0a70-438c-8355-c9cc0f34943f",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed5e591-b756-47f7-b354-06d02465f62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# step 1 : Import the necessary libraries:\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# step 2 : Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# step 3 : Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# step 4 :  Create and train the SVM model with a polynomial kernel\n",
    "svm = SVC(kernel='poly', degree=3)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# step 5 : Make predictions on the testing set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# step 6 :  Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f020318-d014-42da-93fb-5d8a181d4971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec2cbdca-833c-4006-a185-cec6eed88abb",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e6841-4dc6-4fb5-941f-fd2a234fe1e9",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), epsilon is a hyperparameter that defines the margin of tolerance for errors. It determines the width of the margin within which errors are considered acceptable.\n",
    "\n",
    "The number of support vectors in SVR can be influenced by the value of epsilon. When epsilon is increased, allowing a larger margin of tolerance, it generally leads to an increase in the number of support vectors.\n",
    "\n",
    "Here's the intuition behind this relationship:\n",
    "\n",
    "Smaller epsilon: When the value of epsilon is small, it indicates a smaller margin of tolerance for errors. In this case, SVR aims to find a tight fit to the training data. As a result, only the data points close to the true function or within the narrow margin will be considered support vectors. This leads to a smaller number of support vectors.\n",
    "\n",
    "Larger epsilon: When the value of epsilon is increased, it allows for a larger margin of tolerance for errors. SVR becomes more flexible and permits a broader margin around the true function. As a result, data points farther away from the true function or outside the wider margin can still be considered support vectors. This leads to a larger number of support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0ae9b-ba19-4d5e-ba13-e18096794a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a8db2b7-5c8a-4266-855e-bfecc8869b5d",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb2f8c-0508-41ea-a36e-23fbd0eaddf2",
   "metadata": {},
   "source": [
    "The choice of kernel function, C parameter, epsilon parameter, and gamma parameter in Support Vector Regression (SVR) can significantly impact the performance of the model. Let's discuss each parameter and how it affects SVR:\n",
    "\n",
    "Kernel Function:\n",
    "The kernel function determines the type of mapping applied to the input space. Different kernel functions capture different types of relationships between the input and output variables. Commonly used kernel functions in SVR include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n",
    "a.Linear Kernel: Suitable for linear relationships between variables. It performs well when the data is linearly separable or exhibits a linear trend.\n",
    "\n",
    "b.Polynomial Kernel: Captures polynomial relationships between variables. It allows for more flexibility in modeling non-linear relationships, with the degree parameter controlling the degree of the polynomial.\n",
    "\n",
    "c.RBF Kernel: Suitable for non-linear and complex relationships. It can capture intricate patterns in the data, but it requires careful tuning of the gamma parameter.\n",
    "\n",
    "d.Sigmoid Kernel: Useful for modeling non-linear relationships with a sigmoid-like shape.\n",
    "The choice of kernel function depends on the underlying relationships in the data. Experimentation and domain knowledge are essential in selecting the most appropriate kernel function.\n",
    "\n",
    "C Parameter (Regularization Parameter):\n",
    "The C parameter controls the trade-off between fitting the training data and allowing deviations or errors. It determines the penalty for errors made by the SVR model.\n",
    "A small C value allows for more deviations or errors in the model, resulting in a wider margin and a more flexible fit.\n",
    "A large C value enforces a stricter fit to the training data, minimizing errors, and potentially leading to overfitting if the data is noisy or contains outliers.\n",
    "To decide the value of C, consider the noise level in the data and the balance between model complexity and generalization.\n",
    "\n",
    "Epsilon Parameter:\n",
    "The epsilon parameter determines the margin of tolerance for errors. It sets the width of the margin within which errors are considered acceptable.\n",
    "A smaller epsilon value constrains the model to have a smaller margin and be more sensitive to errors, leading to a tighter fit to the data.\n",
    "A larger epsilon value allows for a wider margin and more tolerance for errors, resulting in a looser fit.\n",
    "The choice of epsilon depends on the desired tolerance for errors and the acceptable level of flexibility in the model.\n",
    "\n",
    "Gamma Parameter:\n",
    "The gamma parameter controls the influence of individual training samples in the SVR model. It determines the reach of each training example in the input space.\n",
    "A small gamma value makes the influence of each training example more widespread, resulting in a smoother and more generalized model.\n",
    "A large gamma value restricts the influence of each training example to a smaller region, resulting in a more localized and detailed model.\n",
    "The choice of gamma depends on the complexity of the data and the desired level of flexibility in capturing the relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9735e-e17b-4610-9484-3ff3e0316ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41097cc2-3808-43f6-a7ea-482b82b1ddd8",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg\n",
    "L Split the dataset into training and testing setZ\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "L hse the trained classifier to predict the labels of the testing datW\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "L Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a9bd00f-c932-4868-bd40-3b6f5129758a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n",
      "{'C': 0.1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25)\n",
    "\n",
    "# Preprocess the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "clf = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1.0, 10.0], 'kernel': ['linear', 'rbf']}\n",
    "clf_gs = GridSearchCV(clf, param_grid, scoring='accuracy', cv=5)\n",
    "clf_gs.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(clf_gs.best_params_)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "clf_gs.fit(iris.data, iris.target)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "with open('svm_classifier.pkl', 'wb') as f:\n",
    "     pickle.dump(clf_gs, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb890f-94fd-4870-aa7c-3fe1fc394b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
